{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f79d910e",
      "metadata": {
        "id": "f79d910e"
      },
      "source": [
        "                                DEEP LEARNING PROJECT - STORY TELLING BOT - WORDS TO WORDS - V2                               ▕⃝⃤\n",
        "---\n",
        "For the past years, the feeling of loneliness has become more important in our society due to the lockdown induced by the COVID-19 pandemic. Thus, many of us have find comfort in reading books. However, this comfort was only short-lived as many of the books ending left us on cliff-hangers or maybe unsatisfied regarding the love story of the main characters. Therefore, a solution to this problem would be to create a story telling bot (**SBOT** for short) that would allow us to extrapolate the rest of the story or even feed our imagination with an unlikely romance between the main and some background characters. In fact, who has never wanted to know more about the forbidden love between Harry Potter and Dobby the house elf ? Or maybe explore the day to day life of Ron Weasley and Hermione Granger as a married couple ?\n",
        "\n",
        "\n",
        "\n",
        "> Authors : **Axelle Schyns**, **Lucie Navez** and **Victor Mangeleer**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hw_q5VJLTwlZ",
      "metadata": {
        "id": "hw_q5VJLTwlZ"
      },
      "source": [
        "                                                          Before you start !\n",
        "\n",
        "\n",
        "\n",
        "Everyting you need for the project is freely available using this [link](https://drive.google.com/drive/u/0/folders/1v0QSu0MAqM63yFb7Rcs7w1LJpndfW34_). There, you will be able to access :\n",
        "\n",
        "- The [theoretical model](https://drive.google.com/drive/folders/1ttg5-byTLQjP8-XNAkF7v-QsGxZKgN4L?usp=sharing) of our SBOT.\n",
        "\n",
        "- The different [models](https://drive.google.com/drive/folders/1F0jvLfWxms3RpwknnOUBGWsIculgOMfW?usp=sharing) obtained throughout our training sessions.\n",
        "\n",
        "- The [books](https://drive.google.com/drive/folders/12l2gGdNfpkBaIwETFEtRM4-WmNH_18pG?usp=sharing) used to find useful insigths regarding RNN and how to code them.\n",
        "\n",
        "- The [Harry Potter books](https://drive.google.com/drive/folders/1IxTdwvTO3mk7fuehgV_DVLQzOzwvgImw?usp=sharing) in a .txt format used for the training of the neural network.\n",
        "\n",
        "- The building blocks of our [embedder](https://drive.google.com/drive/u/0/folders/1xFYFTdkK7W9NtaYRdCdziG_VCf0Favbi) named GLOVE. If you'd like to download the complete setup for GLOVE, it is possible using this [download link](http://nlp.stanford.edu/data/wordvecs/glove.6B.zip) (1.1gb).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ySbCpiIIdetz",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-03T12:14:06.190276Z",
          "iopub.status.busy": "2022-05-03T12:14:06.190042Z",
          "iopub.status.idle": "2022-05-03T12:14:06.193836Z",
          "shell.execute_reply": "2022-05-03T12:14:06.193186Z",
          "shell.execute_reply.started": "2022-05-03T12:14:06.190253Z"
        },
        "id": "ySbCpiIIdetz"
      },
      "outputs": [],
      "source": [
        "# Define the path to the user's folder to save all the results\n",
        "user_path = 'results'\n",
        "\n",
        "# Define the platform used for the training\n",
        "gradient = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yc4DZxgIDJcy",
      "metadata": {
        "id": "Yc4DZxgIDJcy"
      },
      "source": [
        "                                                       Parameters of the SBOT\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52SVpmgiZsZh",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-03T12:14:08.212821Z",
          "iopub.status.busy": "2022-05-03T12:14:08.212356Z",
          "iopub.status.idle": "2022-05-03T12:14:08.218730Z",
          "shell.execute_reply": "2022-05-03T12:14:08.218177Z",
          "shell.execute_reply.started": "2022-05-03T12:14:08.212794Z"
        },
        "id": "52SVpmgiZsZh"
      },
      "outputs": [],
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "#                              TUNING PARAMETERS\n",
        "#-------------------------------------------------------------------------------\n",
        "# Define if the SBOT uses attention\n",
        "use_attention = True\n",
        "\n",
        "# Define if the encoder's architecture is uni-directionnal or bi-directionnal.\n",
        "bidirectional = True\n",
        "\n",
        "# Size of the recurrent states (h should be somewhere in [5, 64])\n",
        "hidden_size = 2048\n",
        "\n",
        "# Define the probability that the exact word is used as input for the decoder\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "# Number of epochs\n",
        "nb_epoch = 200\n",
        "\n",
        "# Define the checkpoints that will be used to save the model at different states\n",
        "checkpoints = [10, 25, 50, 75, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, nb_epoch]\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "#                                      DATA\n",
        "#-------------------------------------------------------------------------------\n",
        "# Define the books that will be used for training\n",
        "Books_training = [1, 4, 5, 6, 7, 8, 9]\n",
        "\n",
        "# Define the number of words used as inputs (to capture the context)\n",
        "input_size  = 8\n",
        "\n",
        "# Define the minimum sentence length in the dataset\n",
        "min_sentence_length = 10\n",
        "\n",
        "# Define the maximum sentence length in the dataset\n",
        "max_sentence_length = 30\n",
        "\n",
        "# Define the number of sentences to concatenate in the dataset\n",
        "Paragraph_size = 1\n",
        "\n",
        "# Define the percentage of the book that will be used as a train set\n",
        "train_size  = 90\n",
        "\n",
        "# Define the percentage of the book that will be used as a validation set\n",
        "valid_size  = 10\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "#                               SBOT and TRAINING\n",
        "#-------------------------------------------------------------------------------\n",
        "# Size of the bach in the dataloader\n",
        "batch_size = 64\n",
        "\n",
        "# Define the learning rate of ADAM\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Number of layers in the encoder\n",
        "num_layers_encoder = 2\n",
        "\n",
        "# Number of layers in the decoder\n",
        "num_layers_decoder = 2\n",
        "\n",
        "# Define the probability that a weight is set to 0 (induces randomness)\n",
        "dropout = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tFdafaNqIRa2",
      "metadata": {
        "id": "tFdafaNqIRa2"
      },
      "source": [
        "The different **Harry Potter books** used for trainnig are indexed as follows:\n",
        "\n",
        "\n",
        "Index |Name of the book| Total number of words | Size of the vocabulary\n",
        ":-------------------|:---------------|:---------------:|:---------------:|\n",
        "**1**       | Harry Potter and the philosopher's stone | 78519 | 6112 |\n",
        "**2**       | Harry Potter and the philosopher's stone (**Short version**) | 6520 | 1458 |\n",
        "**3**       | Harry Potter and the philosopher's stone (**Really short version**) | 877 | 376 |\n",
        "**4**       | Harry Potter and the chamber of secret | 86402 | 7313 |\n",
        "**5**       | Harry Potter and the prizonner of Azkaban | 108516 | 8097 |\n",
        "**6**       | Harry Potter and the goblet of fire | 192331 | 11290 |\n",
        "**7**       | Harry Potter and the order of the phoenix | 259635 | 13231 |\n",
        "**8**       | Harry Potter and the half-blood prince | 170682 | 11022 |\n",
        "**9**       | Harry Potter and the deathly hallows | 199561 | 11930 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WdDpT6To57Qx",
      "metadata": {
        "id": "WdDpT6To57Qx"
      },
      "source": [
        "                                                      Into the heart of the SBOT"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nZUjC1T3S9b6",
      "metadata": {
        "id": "nZUjC1T3S9b6"
      },
      "source": [
        "##  SBOT - Initialization\n",
        "In this section, the goal is to first **install** and **import** all the **librairies** needed to create our storry telling bot ! Then, the connection with your **google drive** is established in order to be able to import all the books and data needed for training !"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tqosjvczI9Zo",
      "metadata": {
        "id": "tqosjvczI9Zo"
      },
      "source": [
        "### Librairies - Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1q91Ki18JCVR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-05-03T12:14:09.088003Z",
          "iopub.status.busy": "2022-05-03T12:14:09.087458Z",
          "iopub.status.idle": "2022-05-03T12:14:19.047781Z",
          "shell.execute_reply": "2022-05-03T12:14:19.047193Z",
          "shell.execute_reply.started": "2022-05-03T12:14:09.087976Z"
        },
        "id": "1q91Ki18JCVR",
        "outputId": "8c0af289-9947-4d5a-881f-a2263d2880e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: seaborn in /opt/conda/lib/python3.8/site-packages (0.11.2)\n",
            "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.22.2)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /opt/conda/lib/python3.8/site-packages (from seaborn) (3.5.1)\n",
            "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.6.3)\n",
            "Requirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.3.5)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (3.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (21.3)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (9.0.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (1.3.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (4.29.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2021.3)\n",
            "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.16.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.8/site-packages (2.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.0.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (3.19.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (4.0.1)\n",
            "Requirement already satisfied: gast>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from tensorflow) (59.5.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.22.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.43.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (14.0.1)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (0.25.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (3.6.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.35.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.0.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.26.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.6.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (5.0.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.9)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: keras in /opt/conda/lib/python3.8/site-packages (2.8.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: contractions in /opt/conda/lib/python3.8/site-packages (0.1.72)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /opt/conda/lib/python3.8/site-packages (from contractions) (0.0.21)\n",
            "Requirement already satisfied: pyahocorasick in /opt/conda/lib/python3.8/site-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n",
            "Requirement already satisfied: anyascii in /opt/conda/lib/python3.8/site-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "! pip install seaborn\n",
        "! pip install tensorflow\n",
        "! pip install keras\n",
        "! pip install contractions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f323e18",
      "metadata": {
        "id": "3f323e18"
      },
      "source": [
        "### Librairies - Import\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b84c52f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-05-03T12:14:19.049159Z",
          "iopub.status.busy": "2022-05-03T12:14:19.048982Z",
          "iopub.status.idle": "2022-05-03T12:14:23.301354Z",
          "shell.execute_reply": "2022-05-03T12:14:23.300795Z",
          "shell.execute_reply.started": "2022-05-03T12:14:19.049135Z"
        },
        "id": "b84c52f2",
        "outputId": "bfa90780-7211-4cdd-eecd-c2b06e6e107c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Others\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "import glob\n",
        "import nltk\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import contractions\n",
        "\n",
        "# Pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "#import torchvision\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Keras\n",
        "import keras\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48719bc6",
      "metadata": {
        "id": "48719bc6"
      },
      "source": [
        "### Seting up Google Drive or Gradient\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6456478c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-03T12:14:23.302472Z",
          "iopub.status.busy": "2022-05-03T12:14:23.302305Z",
          "iopub.status.idle": "2022-05-03T12:14:23.306014Z",
          "shell.execute_reply": "2022-05-03T12:14:23.305474Z",
          "shell.execute_reply.started": "2022-05-03T12:14:23.302451Z"
        },
        "id": "6456478c"
      },
      "outputs": [],
      "source": [
        "if not gradient: \n",
        "\n",
        "  # Access to your google driv\n",
        "  from google.colab import drive\n",
        "\n",
        "  # Define your path to the root of the folder\n",
        "  path = \"/content/drive/MyDrive/Deep Learning - SBOT/\"\n",
        "\n",
        "  # Mounting the drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "else:\n",
        "\n",
        "  # Define your path to the root of the folder\n",
        "  path = '/notebooks/root/'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gd9BjZLcAGj9",
      "metadata": {
        "id": "gd9BjZLcAGj9"
      },
      "source": [
        "### Access to the GPUs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hq6Lu5Xh3Cc_",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-03T12:14:23.307415Z",
          "iopub.status.busy": "2022-05-03T12:14:23.307257Z",
          "iopub.status.idle": "2022-05-03T12:14:23.343195Z",
          "shell.execute_reply": "2022-05-03T12:14:23.342633Z",
          "shell.execute_reply.started": "2022-05-03T12:14:23.307397Z"
        },
        "id": "Hq6Lu5Xh3Cc_"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\") \n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8KzsI6EKyQ4",
      "metadata": {
        "id": "a8KzsI6EKyQ4"
      },
      "source": [
        "### Parameters check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5D1or0J-K4Xo",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-03T12:14:23.344624Z",
          "iopub.status.busy": "2022-05-03T12:14:23.344455Z",
          "iopub.status.idle": "2022-05-03T12:14:23.347254Z",
          "shell.execute_reply": "2022-05-03T12:14:23.346825Z",
          "shell.execute_reply.started": "2022-05-03T12:14:23.344603Z"
        },
        "id": "5D1or0J-K4Xo"
      },
      "outputs": [],
      "source": [
        "if input_size >= min_sentence_length:\n",
        "  input_size = min_sentence_length - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JSmC4gng8NV8",
      "metadata": {
        "id": "JSmC4gng8NV8"
      },
      "source": [
        "## SBOT - Functions\n",
        "In this section, all the function that will be used along the notebook are grouped into categories !"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QRIVhKaBa3vV",
      "metadata": {
        "id": "QRIVhKaBa3vV"
      },
      "source": [
        "### Books - Loading and cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XVX_4dm9a8Jx",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-03T12:14:23.348302Z",
          "iopub.status.busy": "2022-05-03T12:14:23.347990Z",
          "iopub.status.idle": "2022-05-03T12:14:23.356676Z",
          "shell.execute_reply": "2022-05-03T12:14:23.356195Z",
          "shell.execute_reply.started": "2022-05-03T12:14:23.348281Z"
        },
        "id": "XVX_4dm9a8Jx"
      },
      "outputs": [],
      "source": [
        "def loadBook(book_path, Dic_contraction):\n",
        "\n",
        "  # Contains the fully prepared book\n",
        "  book_final = []\n",
        "  \n",
        "  # Load the book as a huge string\n",
        "  book_Raw = open(book_path, 'r', encoding= 'utf-8').read()\n",
        "\n",
        "  # Useless thing to be removed (Do not remove \".\" so we can detect sentences)\n",
        "  to_be_removed = [\",\",\"-\",\";\",\":\",\"&\",\"\\\"\",\"”\",\"“\", \"\\n\"]\n",
        "\n",
        "  # Removing ponctuation\n",
        "  for char in to_be_removed:\n",
        "      book_Raw = book_Raw.replace(char, \"\")\n",
        "\n",
        "  # Removing contraction of word\n",
        "  for word in book_Raw.split():\n",
        "    book_Raw = book_Raw.replace(word, contractions.fix(word))\n",
        "\n",
        "  # Splitting the string into sentences\n",
        "  book = nltk.tokenize.sent_tokenize(book_Raw)\n",
        "\n",
        "  # Splitting the sentences into words and adding tokens\n",
        "  for s in book:\n",
        "    s = s.replace(\".\", \"\")\n",
        "    s = s.replace(\"!\", \"\")\n",
        "    s = s.replace(\"?\", \"\")\n",
        "\n",
        "    # Adding sentence and tokens to final book\n",
        "    book_final.append(s.lower().split())\n",
        "\n",
        "  return book_final\n",
        "\n",
        "\n",
        "def bookToDataset(book, inputSize, min_sentence, max_sentence, size_paragraph):\n",
        "\n",
        "  # Contain the transformed book\n",
        "  input  = []\n",
        "  output = []\n",
        "\n",
        "  # Looping over all the sentences\n",
        "  for i in range(len(book) - size_paragraph):\n",
        "\n",
        "    # To concatenate a set of S sentences \n",
        "    sentences = [\"<SOS>\"]\n",
        "\n",
        "    # Concatenates current sentence and the S-1 following sentences \n",
        "    for offset in range(size_paragraph):\n",
        "      sentence = book[i + offset]\n",
        "      for el in sentence:\n",
        "        sentences.append(el)\n",
        "      sentences = sentences + [\"<END>\"]\n",
        "\n",
        "    # Remove start token since it is not part of the real sentence\n",
        "    real_size = len(sentences) - 1\n",
        "\n",
        "    # Removing too short and too long sentences\n",
        "    if  min_sentence < real_size and real_size < max_sentence:\n",
        "\n",
        "      # Completing the end of sentence with nothing to do tokens\n",
        "      nb_NTD = max_sentence - len(sentences)\n",
        "      input.append(sentences[:inputSize])\n",
        "      output.append(sentences[inputSize:] + [\"<NTD>\" for i in range(nb_NTD)])\n",
        "    \n",
        "    else:\n",
        "      continue\n",
        "\n",
        "  return input, output\n",
        "\n",
        "\n",
        "def shufflePairs(X_train, y_train):\n",
        "\n",
        "    # Links the dataset together using zip\n",
        "    linked_set = list(zip(X_train, y_train))\n",
        "\n",
        "    # Shuffle all the elements\n",
        "    random.shuffle(linked_set)\n",
        "\n",
        "    # Reconstruction of the datasets\n",
        "    X_train, y_train = zip(*linked_set)\n",
        "    X_train, y_train = list(X_train), list(y_train)\n",
        "\n",
        "    return X_train, y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qjMgFPsB4GFG",
      "metadata": {
        "id": "qjMgFPsB4GFG"
      },
      "source": [
        "### Books - Conversion from list to tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S__ewXJV4HcG",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-03T12:14:23.357780Z",
          "iopub.status.busy": "2022-05-03T12:14:23.357364Z",
          "iopub.status.idle": "2022-05-03T12:14:23.363982Z",
          "shell.execute_reply": "2022-05-03T12:14:23.363519Z",
          "shell.execute_reply.started": "2022-05-03T12:14:23.357758Z"
        },
        "id": "S__ewXJV4HcG"
      },
      "outputs": [],
      "source": [
        "def inputToTensor(inputs, embedder, embedder_size = 50):\n",
        "  #--------------\n",
        "  # Documentation\n",
        "  #--------------\n",
        "  # This function converts the input for the encoder (a list of \n",
        "  # sentences) into a readable input for the SBOT (tensor of tensors)\n",
        "  #\n",
        "  # inputs        : list of sentences\n",
        "  #\n",
        "  # embedder      : our embedder containing the harry potter vocabulary\n",
        "  #\n",
        "  # embedder_size : the size of the vector produced by the embedder\n",
        "  #\n",
        "  # Information regarding the input\n",
        "  batch_size = len(inputs)\n",
        "  seq_len    = len(inputs[0])\n",
        "\n",
        "  # Stores the converted input\n",
        "  tensor_input = torch.zeros(batch_size, seq_len, embedder_size)\n",
        "\n",
        "  # Looping over the column first\n",
        "  for b in range(batch_size):\n",
        "    \n",
        "    # Current sentence\n",
        "    word_list = inputs[b]\n",
        "\n",
        "    # Converting each word of the sentence\n",
        "    for w in range(len(word_list)):\n",
        "\n",
        "      # Known word by the embedder\n",
        "      if word_list[w] in embedder:\n",
        "        tensor_input[b][w] = torch.tensor(embedder[word_list[w]])\n",
        "\n",
        "      # Unknown word = Unknown token\n",
        "      else:\n",
        "        tensor_input[b][w] = torch.tensor(embedder[\"<UKN>\"])\n",
        "\n",
        "  return tensor_input\n",
        "\n",
        "\n",
        "def getMaskFromList(inputs):\n",
        "\n",
        "  # List of padding tokens to mask\n",
        "  masked = [\"<NTD>\", \"<UKN>\"]\n",
        "\n",
        "  # Information regarding the input\n",
        "  batch_size = len(inputs)\n",
        "  seq_len    = len(inputs[0])\n",
        "\n",
        "  # Stores the converted input\n",
        "  mask = torch.zeros(batch_size, seq_len, dtype = bool)\n",
        "\n",
        "  # Looping over the column first\n",
        "  for b in range(batch_size):\n",
        "    \n",
        "    # Current sentence\n",
        "    word_list = inputs[b]\n",
        "\n",
        "    # Converting each word of the sentence\n",
        "    for w in range(len(word_list)):\n",
        "\n",
        "      # Known word by the embedder\n",
        "      if word_list[w] in masked:\n",
        "        mask[b][w] = False\n",
        "\n",
        "      # Unknown word = Unknown token\n",
        "      else:\n",
        "        mask[b][w] = True\n",
        "\n",
        "  return mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PAl9QgyaXk31",
      "metadata": {
        "id": "PAl9QgyaXk31"
      },
      "source": [
        "### Tensor - Probabilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5mWJ1jP5Xs4G",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-03T12:14:40.142968Z",
          "iopub.status.busy": "2022-05-03T12:14:40.142734Z",
          "iopub.status.idle": "2022-05-03T12:14:40.152337Z",
          "shell.execute_reply": "2022-05-03T12:14:40.151856Z",
          "shell.execute_reply.started": "2022-05-03T12:14:40.142944Z"
        },
        "id": "5mWJ1jP5Xs4G"
      },
      "outputs": [],
      "source": [
        "def WordToId(word, embedder_WordToId):\n",
        "    #--------------\n",
        "    # Documentation\n",
        "    #--------------\n",
        "    # Retrieves from the dictionnary embedder_WordToId the id of the corresponding word\n",
        "    #\n",
        "    return embedder_WordToId.get(word)\n",
        "\n",
        "def IdToWord(Id, embedder_IdToWord):\n",
        "    #--------------\n",
        "    # Documentation\n",
        "    #--------------\n",
        "    # Retrieves from the dictionnary embedder_IdToWord the word corresponding to the given id \n",
        "    #\n",
        "    return embedder_IdToWord[Id.item()]\n",
        "\n",
        "def outputToId(outputs, embedder_WordToId):\n",
        "  #--------------\n",
        "  # Documentation\n",
        "  #--------------\n",
        "  # This function converts a list of words into a tensor of probability vectors (used for CE).\n",
        "  #\n",
        "  # outputs           : list of sentences\n",
        "  #\n",
        "  # embedder_WordToId : the embedder that convert a word to its ID\n",
        "  #\n",
        "  # NOTE : If the word is uknown, all the values are set to 0.\n",
        "  #\n",
        "  # Information regarding the input\n",
        "  batch_size = len(outputs)\n",
        "  seq_len    = len(outputs[0])\n",
        "\n",
        "  # Stores the converted input\n",
        "  tensor_input = torch.zeros(batch_size, seq_len)\n",
        "\n",
        "  # Looping over the column first\n",
        "  for b in range(batch_size):\n",
        "    \n",
        "    # Current sentence\n",
        "    word_list = outputs[b]\n",
        "\n",
        "    # Converting each word of the sentence\n",
        "    for w in range(seq_len):\n",
        "\n",
        "      # Index of the word\n",
        "      word_id = WordToId(word_list[w], embedder_WordToId)\n",
        "      \n",
        "      # If the word is unknown, keep the vector with 0s\n",
        "      if word_id == None:\n",
        "        word_id = WordToId(\"<UKN>\", embedder_WordToId)\n",
        "\n",
        "      # Placing index\n",
        "      tensor_input[b][w] = word_id\n",
        "\n",
        "  tensor_input = tensor_input.type(torch.LongTensor) \n",
        "  \n",
        "  return tensor_input\n",
        "\n",
        "def idToProba(outputs_id, embedder_WordToId):\n",
        "  #--------------\n",
        "  # Documentation\n",
        "  #--------------\n",
        "  # This function converts a list of words into a tensor of probability vectors (used for CE).\n",
        "  #\n",
        "  # outputs           : list of sentences\n",
        "  #\n",
        "  # embedder_WordToId : the embedder that convert a word to its ID\n",
        "  #\n",
        "  # NOTE : If the word is uknown, all the values are set to 0.\n",
        "  #\n",
        "  # Information regarding the input\n",
        "  batch_size = len(outputs_id)\n",
        "  seq_len    = len(outputs_id[0])\n",
        "\n",
        "  # Stores the converted input\n",
        "  tensor_input = torch.zeros(batch_size, seq_len, len(embedder_WordToId))\n",
        "\n",
        "  # Looping over the column first\n",
        "  for b in range(batch_size):\n",
        "    \n",
        "    # Current sentence\n",
        "    word_list = outputs_id[b]\n",
        "\n",
        "    # Converting each word of the sentence\n",
        "    for w in range(seq_len):\n",
        "\n",
        "      # Index of the word\n",
        "      word_id = outputs_id[b][w]\n",
        "      \n",
        "      # Placing index\n",
        "      tensor_input[b][w][int(word_id.item())] = 1\n",
        "\n",
        "  return tensor_input\n",
        "\n",
        "\n",
        "def getPredictedWord(predictions, embedder_IdToWord):\n",
        "    #--------------\n",
        "    # Documentation\n",
        "    #--------------\n",
        "    # This functions converts the prediction made by the SBOT into words\n",
        "    #\n",
        "    # predictions       : output of the SBOT\n",
        "    #\n",
        "    # embedder_IdToWord : embedder dictionnary that gives ID -> Word\n",
        "    #\n",
        "    # Contains ALL the words that have been predicted\n",
        "    words_predicted = []\n",
        "\n",
        "    # Information\n",
        "    batch_size  = len(predictions)\n",
        "    output_size = len(predictions[0])\n",
        "\n",
        "    # Looping over the columns\n",
        "    for b in range(batch_size):\n",
        "\n",
        "      # Contains the words predicted for an element of the batch\n",
        "      words_current = []\n",
        "\n",
        "      # Converting each word of the current sentence\n",
        "      for w in range(output_size):\n",
        "\n",
        "          # Current word\n",
        "          word = predictions[b][w]\n",
        "\n",
        "          # Index of highest probability\n",
        "          word_index = torch.argmax(torch.softmax(word, 0))\n",
        "\n",
        "          # Adding the converted word\n",
        "          words_current.append(IdToWord(word_index, embedder_IdToWord))\n",
        "\n",
        "      # Storing the sentence\n",
        "      words_predicted.append(words_current)\n",
        "\n",
        "    return words_predicted"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Emp62PW1SvTJ",
      "metadata": {
        "id": "Emp62PW1SvTJ"
      },
      "source": [
        "### Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-nz__KJWSyFQ",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-03T12:14:43.094514Z",
          "iopub.status.busy": "2022-05-03T12:14:43.094281Z",
          "iopub.status.idle": "2022-05-03T12:14:43.113638Z",
          "shell.execute_reply": "2022-05-03T12:14:43.113178Z",
          "shell.execute_reply.started": "2022-05-03T12:14:43.094490Z"
        },
        "id": "-nz__KJWSyFQ"
      },
      "outputs": [],
      "source": [
        "def trainingInfo(num_layers_encoder, num_layers_decoder, hidden_size, total_params, teacher_forcing_ratio, bidirectional, nb_epoch, batch_size, attention, nb_input_words):\n",
        "  print(\"---------------------------------------------------------------------\")\n",
        "  print(\"                       SBOT - Training session                       \")\n",
        "  print(\"---------------------------------------------------------------------\")\n",
        "  print(\"----------\")\n",
        "  print(\"Parameters\")\n",
        "  print(\"----------\")\n",
        "  print(\"Number of layers (Encoder) : \" + str(num_layers_encoder))\n",
        "  print(\"\")\n",
        "  print(\"Number of layers (Decoder) : \" + str(num_layers_decoder))\n",
        "  print(\"\")\n",
        "  print(\"Size of reccurent states   : \" + str(hidden_size))\n",
        "  print(\" \")\n",
        "  print(\"Number of parameters       : \" + str(total_params))\n",
        "  print(\" \")\n",
        "  print(\"Attention                  : \" + str(attention))\n",
        "  print(\" \")\n",
        "  print(\"Nb. Inputs Words           : \" + str(nb_input_words))\n",
        "  print(\" \")\n",
        "  print(\"Teacher forcing ratio      : \" + str(teacher_forcing_ratio))\n",
        "  print(\" \")\n",
        "  print(\"Bidirectional              : \" + str(bidirectional))\n",
        "  print(\" \")\n",
        "  print(\"Number of epochs           : \" + str(nb_epoch))\n",
        "  print(\" \")\n",
        "  print(\"Size of the batch          : \" + str(batch_size))\n",
        "  print(\" \")\n",
        "  print(\"-------\")\n",
        "  print(\"Traning\")\n",
        "  print(\"-------\")\n",
        "\n",
        "\n",
        "def progressBar(loss_training, loss_validation, estimated_time, percent, width = 40):\n",
        "\n",
        "    # Setting up the useful information\n",
        "    left  = width * percent // 100\n",
        "    right = width - left\n",
        "    tags = \"#\" * int(left)\n",
        "    spaces = \" \" * int(right)\n",
        "    percents = f\"{percent:.2f} %\"\n",
        "    loss_training = f\"{loss_training * 1:.6f}\"\n",
        "    loss_validation = f\"{loss_validation * 1:.6f}\"\n",
        "    estimated_time = f\"{estimated_time:.2f} s\"\n",
        "\n",
        "    # Displaying a really cool progress bar !\n",
        "    print(\"\\r[\", tags, spaces, \"] - \", percents, \" | Loss (Training) = \", loss_training, \" | Loss (Validation) = \", \n",
        "          loss_validation,  \" | Time left : \", estimated_time ,sep=\"\", end=\"\", flush = True)\n",
        "\n",
        "\n",
        "def saveTrainingInfo(path, loss_train, loss_valid, parameters):\n",
        "  \n",
        "    # Creation of the dictionnary story all the results\n",
        "    results = {\n",
        "        \"Loss Train\"       : loss_train,\n",
        "        \"Loss Valid\"       : loss_valid,\n",
        "        \"Layers Encoder\"   : parameters[0],\n",
        "        \"Layers Decoder\"   : parameters[1],\n",
        "        \"Hidden Size\"      : parameters[2],\n",
        "        \"Total Parameters\" : parameters[3],\n",
        "        \"Attention\"        : parameters[4],\n",
        "        \"Input Words\"      : parameters[5],\n",
        "        \"Teacher Forcing\"  : parameters[6],\n",
        "        \"Bidirectional\"    : parameters[7],\n",
        "        \"Number of epochs\" : parameters[8],\n",
        "        \"Batch Size\"       : parameters[9],\n",
        "      }\n",
        "\n",
        "    # Creation of the model's name\n",
        "    number_models = glob.glob(folder_path + \"*.pth\")\n",
        "    results_name  = \"SBOTV2_\" + str(len(number_models)) + \"_Parameters.pkl\"\n",
        "\n",
        "    # Saving the dictionnary\n",
        "    new_dic = open(path + results_name, \"wb\")\n",
        "\n",
        "    # write the python object (dict) to pickle file\n",
        "    pickle.dump(results, new_dic)\n",
        "\n",
        "    # close file\n",
        "    new_dic.close()\n",
        "\n",
        "\n",
        "def loadTrainingInfo(path, model_index):\n",
        "\n",
        "    # Charging the dictionnary in pickle format\n",
        "    results_pkl = open(path + \"SBOTV2_\" + str(model_index) + \"_Parameters.pkl\", \"rb\")\n",
        "\n",
        "    # Loading the dictionnary\n",
        "    results = pickle.load(results_pkl)\n",
        "\n",
        "    return results\n",
        "\n",
        "def loadTrainingParameters(results):\n",
        "\n",
        "    parameters = []\n",
        "    parameters.append(results[\"Loss Train\"])\n",
        "    parameters.append(results[\"Loss Valid\"])\n",
        "    parameters.append(results[\"Layers Encoder\"])\n",
        "    parameters.append(results[\"Layers Decoder\"])\n",
        "    parameters.append(results[\"Hidden Size\"])\n",
        "    parameters.append(results[\"Total Parameters\"])\n",
        "    parameters.append(results[\"Attention\"])\n",
        "    parameters.append(results[\"Input Words\"])\n",
        "    parameters.append(results[\"Teacher Forcing\"])\n",
        "    parameters.append(results[\"Bidirectional\"])\n",
        "    parameters.append(results[\"Number of epochs\"])\n",
        "    parameters.append(results[\"Batch Size\"])\n",
        "\n",
        "    return parameters\n",
        "\n",
        "def saveModel(path, model, optimizer):\n",
        "\n",
        "    # Creation of the model's name\n",
        "    model_number = glob.glob(folder_path + \"*.pth\")\n",
        "    model_name   = \"SBOTV2_\" + str(len(model_number)) + \".pth\"\n",
        "\n",
        "    # Saves all the state information\n",
        "    checkpoint = {\n",
        "    'state_dict': model.state_dict(),\n",
        "    'optimizer' : optimizer.state_dict()\n",
        "    }\n",
        "\n",
        "    # Saving everything everything\n",
        "    torch.save(checkpoint, path + model_name)\n",
        "\n",
        "\n",
        "def loadModel(path, model, optimizer, model_index):\n",
        "\n",
        "    # Loading the corresponding checkpoint\n",
        "    checkpoint = torch.load(path + \"SBOTV2_\" + str(model_index) + \".pth\")\n",
        "\n",
        "    # Transfering the parameters\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "\n",
        "    return model, optimizer\n",
        "\n",
        "\n",
        "def showModel(model_index, parameters_model, loss_train_model, loss_valid_model):\n",
        "    print(\"----------------------------------------------------\")\n",
        "    print(\"                 Model's information                \")\n",
        "    print(\"----------------------------------------------------\\n\")\n",
        "\n",
        "    # Displaying the parameters\n",
        "    print(\"Model Name       : SBOTV2_\" + str(model_index) + \"\\n\\n\")\n",
        "\n",
        "    for p in parameters_model:\n",
        "      print(p)\n",
        "\n",
        "    nb_epoch_vector = [i for i in range(len(loss_train_model))]\n",
        "\n",
        "    # Changing plot style\n",
        "    sns.set_style(\"darkgrid\")\n",
        "\n",
        "    # Training loss\n",
        "    plt.figure()\n",
        "    plt_training = sns.lineplot(nb_epoch_vector, loss_train_model)\n",
        "    plt_training.set_xlabel(\"Number of epochs\", fontsize = 20)\n",
        "    plt_training.set_ylabel(\"Training loss\", fontsize = 20)\n",
        "\n",
        "    plt.figure()\n",
        "    plt_training = sns.lineplot(nb_epoch_vector, loss_valid_model)\n",
        "    plt_training.set_xlabel(\"Number of epochs\", fontsize = 20)\n",
        "    plt_training.set_ylabel(\"Validation loss\", fontsize = 20)\n",
        "\n",
        "\n",
        "def testModel(model, text, sentence_length, device):\n",
        "  print(\"----------------------------------------------------\")\n",
        "  print(\"                 Generated Text                     \")\n",
        "  print(\"----------------------------------------------------\\n\")\n",
        "\n",
        "  # Preparation of the text\n",
        "  text = text.lower()\n",
        "\n",
        "  # Useless thing to be removed (Do not remove \".\" so we can detect sentences)\n",
        "  to_be_removed = [\",\",\"-\",\"!\",\";\",\":\",\"?\",\"&\",\"\\\"\",\"”\",\"“\", \"\\n\"]\n",
        "\n",
        "  # Removing ponctuation\n",
        "  for char in to_be_removed:\n",
        "      text_corrected = text.replace(char, \"\")\n",
        "\n",
        "  text_corrected = text.replace(\".\", \" <END>\")\n",
        "  text_corrected = text_corrected.split()\n",
        "  text_corrected = [\"<SOS>\"] + text_corrected\n",
        "  text_corrected = [text_corrected]\n",
        "  text_input     = inputToTensor(text_corrected, embedder).to(device)\n",
        "\n",
        "  # Making predictions\n",
        "  pred = model(text_input, sentence_length, embedder, embedder_IdToWord, training_mode = False)\n",
        "\n",
        "  # Creation of the full text paragraph\n",
        "  index = 0\n",
        "  sentence = \"\"\n",
        "\n",
        "  for p in pred:\n",
        "\n",
        "      current_word = p[0][0]\n",
        "\n",
        "      # Checks the current word expression\n",
        "      if current_word == \"<END>\":\n",
        "        current_word = \".\"\n",
        "  \n",
        "      if current_word == \"<NTD>\":\n",
        "        current_word = \"\"\n",
        "        continue\n",
        "\n",
        "      # Append the final setence\n",
        "      if index < 10:\n",
        "        sentence = sentence + current_word + \" \"\n",
        "        index = index + 1\n",
        "      else:\n",
        "        sentence = sentence + current_word + \"\\n\"\n",
        "        index = 0\n",
        "\n",
        "  return sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2_mtU9uia8ry",
      "metadata": {
        "id": "2_mtU9uia8ry"
      },
      "source": [
        "### Others"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q4VWGj61a_zy",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-03T12:14:46.404354Z",
          "iopub.status.busy": "2022-05-03T12:14:46.403702Z",
          "iopub.status.idle": "2022-05-03T12:14:46.411546Z",
          "shell.execute_reply": "2022-05-03T12:14:46.411092Z",
          "shell.execute_reply.started": "2022-05-03T12:14:46.404324Z"
        },
        "id": "q4VWGj61a_zy"
      },
      "outputs": [],
      "source": [
        "def Harry_Logo():\n",
        "  print(\"                                         _ __\")\n",
        "  print(\"        ___                             | '  \\ \")\n",
        "  print(\"   ___  \\ /  ___         ,'\\_           | .-. \\        /|\")\n",
        "  print(\"   \\ /  | |,'__ \\  ,'\\_  |   \\          | | | |      ,' |_   /|\")\n",
        "  print(\" _ | |  | |\\/  \\ \\ |   \\ | |\\_|    _    | |_| |   _ '-. .-',' |_   _\")\n",
        "  print(\"// | |  | |____| | | |\\_|| |__    //    |     | ,'_`. | | '-. .-',' `. ,'\\_\")\n",
        "  print(\" \\\\_| |_,' .-, _  | | |   | |\\ \\  //    .| |\\_/ | / \\ || |   | | / |\\  \\|   \\ \")\n",
        "  print(\" `-. .-'| |/ / | | | |   | | \\ \\//     |  |    | | | || |   | | | |_\\ || |\\_|\")\n",
        "  print(\"   | |  | || \\_| | | |   /_\\  \\ /      | |`    | | | || |   | | | .---'| |\")\n",
        "  print(\"   | |  | |\\___,_\\ /_\\ _      //       | |     | \\_/ || |   | | | |  /\\| |\")\n",
        "  print(\"   /_\\  | |           //_____//       .||`  _   `._,' | |   | | \\ `-' /| |\")\n",
        "  print(\"        /_\\           `------'        \\ |             `.\\   | |  `._,' /_\\ \")\n",
        "  print(\" \")\n",
        "  print(\" \")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-SATueBMMv4R",
      "metadata": {
        "id": "-SATueBMMv4R"
      },
      "source": [
        "## SBOT - Books and GLOVE\n",
        "In this section, all the books are loaded into the notebook as well as the glove embedder. Therefore, one must only **load once** this section ! "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sun1vN_gdbU6",
      "metadata": {
        "id": "sun1vN_gdbU6"
      },
      "source": [
        "### Loading the books"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hr_uggXWdr7d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-03T12:14:47.778997Z",
          "iopub.status.busy": "2022-05-03T12:14:47.778764Z"
        },
        "id": "Hr_uggXWdr7d"
      },
      "outputs": [],
      "source": [
        "# Loading dictionnary to replace contractions\n",
        "HP_dictionnary = open(path + \"Glove/Glove_HarryPotter/HP_Contraction.pkl\", \"rb\")\n",
        "\n",
        "# Loading the dictionnary\n",
        "HP_contraction = pickle.load(HP_dictionnary)\n",
        "\n",
        "# Path to the books\n",
        "book_11 = path + \"Data/HP1.txt\"\n",
        "book_12 = path + \"Data/HP1_Dummy.txt\"\n",
        "book_13 = path + \"Data/HP1_Really_Dummy.txt\"\n",
        "book_2  = path + \"Data/HP2.txt\"\n",
        "book_3  = path + \"Data/HP3.txt\"\n",
        "book_4  = path + \"Data/HP4.txt\"\n",
        "book_5  = path + \"Data/HP5.txt\"\n",
        "book_6  = path + \"Data/HP6.txt\"\n",
        "book_7  = path + \"Data/HP7.txt\"\n",
        "\n",
        "# Loading the books\n",
        "book_11 = loadBook(book_11, HP_contraction)\n",
        "book_12 = loadBook(book_12, HP_contraction)\n",
        "book_13 = loadBook(book_13, HP_contraction)\n",
        "book_2  = loadBook(book_2,  HP_contraction)\n",
        "book_3  = loadBook(book_3,  HP_contraction)\n",
        "book_4  = loadBook(book_4,  HP_contraction)\n",
        "book_5  = loadBook(book_5,  HP_contraction)\n",
        "book_6  = loadBook(book_6,  HP_contraction)\n",
        "book_7  = loadBook(book_7,  HP_contraction)\n",
        "\n",
        "# Grouping all the information regarding the books\n",
        "HP_books  = [book_11,  book_12, book_13, \n",
        "             book_2 ,  book_3 ,  book_4, \n",
        "             book_5 ,  book_6 ,  book_7]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YKbgkbKv6vwT",
      "metadata": {
        "id": "YKbgkbKv6vwT"
      },
      "source": [
        "### Loading GLOVE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3P68TYt2oTQd",
      "metadata": {
        "id": "3P68TYt2oTQd"
      },
      "outputs": [],
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "#\n",
        "#                               EMBEDDER TOKENS\n",
        "#\n",
        "#-------------------------------------------------------------------------------\n",
        "# Start token (used as first input for the decoder)\n",
        "sos_value = 1\n",
        "\n",
        "# Unknown value (used if the words is unknown to the embedder)\n",
        "ukn_value = 0.75\n",
        "\n",
        "# Nothing to do (used to complete output if not enough words)\n",
        "ntd_value = 0.25\n",
        "\n",
        "# End token (used as last ouput of the decoder)\n",
        "eos_value = 0\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "#\n",
        "#                              LOADING THE EMBEDDER\n",
        "#\n",
        "#-------------------------------------------------------------------------------\n",
        "# Opening the file on Google Drive\n",
        "HP_dictionnary = open(path + \"Glove/Glove_HarryPotter/HP_Dictionnary.pkl\", \"rb\")\n",
        "\n",
        "# Loading the dictionnary\n",
        "embedder = pickle.load(HP_dictionnary)\n",
        "\n",
        "# Adding missing tokens\n",
        "embedder[\"<SOS>\"] = np.ones(50) * sos_value\n",
        "embedder[\"<END>\"] = np.ones(50) * eos_value\n",
        "embedder[\"<NTD>\"] = np.ones(50) * ntd_value\n",
        "embedder[\"<UKN>\"] = np.ones(50) * ukn_value\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "#\n",
        "#                     CREATION OF REVERSE EMBEDDER (ID to WORD)\n",
        "#\n",
        "#-------------------------------------------------------------------------------\n",
        "# Creation of the ID -> Word dictionnary\n",
        "embedder_WordToId = dict()\n",
        "cpt = 0\n",
        "\n",
        "# Retrieves each possible word \n",
        "for d in embedder:\n",
        "\n",
        "  # Encodes the pair (d, cpt) in the dictionnary \n",
        "  embedder_WordToId[d] = cpt\n",
        "  cpt += 1\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "#\n",
        "#                   CREATION OF REVERSE EMBEDDER 2 (WORD to ID)\n",
        "#\n",
        "#-------------------------------------------------------------------------------\n",
        "# Creation of the ID -> Word dictionnary\n",
        "embedder_IdToWord = dict()\n",
        "cpt = 0\n",
        "\n",
        "# Retrieves each possible word \n",
        "for d in embedder:\n",
        "\n",
        "  # encode the pair (cpt, d) in the dictionnary \n",
        "  embedder_IdToWord[cpt] = d\n",
        "  cpt += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ke6Xx20OaPSy",
      "metadata": {
        "id": "ke6Xx20OaPSy"
      },
      "source": [
        "## SBOT - Creation of the dataset\n",
        "In this section, the purpose is to create the dataloader for the training, validation and test sets. Therefore, if **some parameters are changed**, one must always **re-run this section** !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HKUtTsFcQjKi",
      "metadata": {
        "id": "HKUtTsFcQjKi"
      },
      "outputs": [],
      "source": [
        "# Used to easily store our datasets\n",
        "class HPDataset(Dataset):\n",
        "  def __init__(self, x, y, y_p, y_m):\n",
        "      self.x = x\n",
        "      self.y = y\n",
        "      self.y_proba = y_p\n",
        "      self.y_mask  = y_m\n",
        "      \n",
        "  def __getitem__(self, index):\n",
        "      return (self.x[index, :, :], self.y[index, :, :], self.y_proba[index, :],  self.y_mask[index, :])\n",
        "  \n",
        "  def __len__(self):\n",
        "      return self.x.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2OtWDIpwQFPd",
      "metadata": {
        "id": "2OtWDIpwQFPd"
      },
      "outputs": [],
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "#\n",
        "#                     Creation of the dataset as lists\n",
        "#\n",
        "#-------------------------------------------------------------------------------\n",
        "# Contains the dataset but still in string format\n",
        "X = []\n",
        "X_train = [] \n",
        "X_valid = []\n",
        "\n",
        "y = []\n",
        "y_train = []\n",
        "y_valid = []\n",
        "\n",
        "for i in range(len(Books_training)):\n",
        "\n",
        "    # Current book, size and max id\n",
        "    book = HP_books[Books_training[i] - 1]\n",
        "\n",
        "    # Creation of input/output\n",
        "    X_curr, y_curr = bookToDataset(book, input_size, min_sentence_length, max_sentence_length, Paragraph_size)\n",
        "    X = X + X_curr\n",
        "    y = y + y_curr\n",
        "\n",
        "# Shuffling everything ! (Better for generalization)\n",
        "X, y = shufflePairs(X, y)\n",
        "\n",
        "# Dimensions of the sets\n",
        "index_train = int(len(X) * (train_size/100))\n",
        "\n",
        "# Creation of the sets\n",
        "X_train = X_train + X[:index_train]\n",
        "X_valid = X_valid + X[index_train:]\n",
        "\n",
        "y_train = y_train + y[:index_train]\n",
        "y_valid = y_valid + y[index_train:]\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "#                           Conversion of the masks\n",
        "#-------------------------------------------------------------------------------\n",
        "mask_train = getMaskFromList(y_train)\n",
        "mask_valid = getMaskFromList(y_valid)\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "#                     Conversion of lists to torch vectors\n",
        "#-------------------------------------------------------------------------------\n",
        "X_train_torch = inputToTensor(X_train, embedder)\n",
        "y_train_torch = inputToTensor(y_train, embedder)\n",
        "\n",
        "X_valid_torch = inputToTensor(X_valid, embedder)\n",
        "y_valid_torch = inputToTensor(y_valid, embedder)\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "#                 Conversion of lists to probability vectors\n",
        "#-------------------------------------------------------------------------------\n",
        "y_train_prob = outputToId(y_train, embedder_WordToId)\n",
        "y_valid_prob = outputToId(y_valid, embedder_WordToId)\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "#                           Creation of the dataloader\n",
        "#-------------------------------------------------------------------------------\n",
        "dataset_train = HPDataset(X_train_torch, y_train_torch, y_train_prob, mask_train)\n",
        "dataset_valid = HPDataset(X_valid_torch, y_valid_torch, y_valid_prob, mask_valid)\n",
        "\n",
        "HP_train  = DataLoader(dataset_train, batch_size = batch_size)\n",
        "HP_valid  = DataLoader(dataset_valid, batch_size = batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VzoeJ98gpqEr",
      "metadata": {
        "id": "VzoeJ98gpqEr"
      },
      "source": [
        "## SBOT - Architecture\n",
        "\n",
        "In this section, we are going to first build the architecture of our SBOT by defining the **encoder-decoder** and the **attention mechanisms** !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-G8yVwWjqGw8",
      "metadata": {
        "id": "-G8yVwWjqGw8"
      },
      "outputs": [],
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "#                                   ENCODER\n",
        "#-------------------------------------------------------------------------------\n",
        "class Encoder_SBOT(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hid_dimensions, num_layers, dropout = 0, bidirectional = False):\n",
        "        super(Encoder_SBOT, self).__init__()\n",
        "\n",
        "        self.gru = nn.GRU(input_size, hid_dimensions, num_layers = num_layers, dropout = dropout, bidirectional = bidirectional, batch_first = True)\n",
        "        self.bidirectional = bidirectional \n",
        "        \n",
        "    def forward(self, x): \n",
        "         \n",
        "        outputs, hidden = self.gru(x)\n",
        "\n",
        "        if self.bidirectional == True: \n",
        "          h1 = hidden[-2]\n",
        "          h2 = hidden[-1]\n",
        "          hidden = torch.cat((h1, h2), dim = 1)\n",
        "          hidden = torch.unsqueeze(hidden, 0)\n",
        "        else:\n",
        "          hidden = hidden[-1]\n",
        "          hidden = torch.unsqueeze(hidden, 0)\n",
        "\n",
        "        return outputs, hidden\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "#                                    DECODER\n",
        "#-------------------------------------------------------------------------------\n",
        "class Decoder_SBOT(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hid_dimensions, num_layers, output_size, dropout):\n",
        "        super(Decoder_SBOT, self).__init__()\n",
        "\n",
        "        self.gru = nn.GRU(input_size, hid_dimensions, num_layers = num_layers, dropout = dropout, batch_first = True)\n",
        "        self.mlp = nn.Linear(hid_dimensions, output_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \n",
        "        output, hidden = self.gru(x, hidden)\n",
        "        output = self.mlp(output)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "#                                    ATTENTION\n",
        "#-------------------------------------------------------------------------------\n",
        "class Attention_SBOT(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size):\n",
        "        super(Attention_SBOT, self).__init__()\n",
        "\n",
        "        # Used to compute weights\n",
        "        self.SM = nn.Softmax(dim = 1)\n",
        "\n",
        "    def forward(self, output_encoder, hidden_decoder):\n",
        "\n",
        "        # Transformation of hidden (Batch first, vector product)\n",
        "        hidden_decoder = torch.transpose(hidden_decoder, 0, 1)\n",
        "        hidden_decoder = torch.transpose(hidden_decoder, 1, 2)\n",
        "          \n",
        "        # Computing the score (e_{i,j}, vector product)\n",
        "        score = torch.bmm(output_encoder, hidden_decoder)\n",
        "        \n",
        "        # Final preparation of the weights\n",
        "        weights = torch.transpose(self.SM(score), 1, 2)\n",
        "\n",
        "        # Computing attention reccurent layer\n",
        "        hidden_attention = torch.transpose(torch.bmm(weights, output_encoder), 0, 1)\n",
        "\n",
        "        return hidden_attention.contiguous()\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "#                                    SBOT\n",
        "#-------------------------------------------------------------------------------\n",
        "class SBOT_V2(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers_encoder, \n",
        "                 num_layers_decoder, device, dropout, bidirectional, use_attention, nb_input_words):\n",
        "        super(SBOT_V2, self).__init__()\n",
        "\n",
        "        # Initialization of the main components\n",
        "        self.encoder = Encoder_SBOT(input_size, hidden_size, num_layers_encoder, dropout, bidirectional).to(device)\n",
        "\n",
        "        if bidirectional == True:\n",
        "          self.decoder = Decoder_SBOT(input_size, hidden_size * 2, num_layers_decoder, output_size, dropout).to(device)\n",
        "        else:\n",
        "          self.decoder = Decoder_SBOT(input_size, hidden_size, num_layers_decoder, output_size, dropout).to(device)\n",
        "\n",
        "        self.attention = Attention_SBOT(nb_input_words).to(device)\n",
        "\n",
        "        # Other parameters\n",
        "        self.device = device\n",
        "        self.use_att = use_attention\n",
        "        self.num_layers_decoder = num_layers_decoder\n",
        "\n",
        "    def forward(self, input, output_size, embedder, embedder_IdToWord, outputs = [], teacher_forcing_ratio = 0, training_mode = False):\n",
        "\n",
        "        # Stores all the predictions made by the SBOT\n",
        "        predictions = torch.zeros(len(input), output_size, len(embedder_IdToWord)).to(self.device)\n",
        "\n",
        "        #-----------------------------------------------------------------------\n",
        "        #                               ENCODING\n",
        "        #-----------------------------------------------------------------------\n",
        "        out_encoder, hidden = self.encoder(input)\n",
        "\n",
        "        #-----------------------------------------------------------------------\n",
        "        #                               ATTENTION\n",
        "        #-----------------------------------------------------------------------\n",
        "        if self.use_att == True:\n",
        "          hidden = self.attention(out_encoder, hidden)\n",
        "\n",
        "        # Preparation of hidden vector if decoder's layers > 1\n",
        "        hidden = hidden.repeat(self.num_layers_decoder, 1, 1)\n",
        "\n",
        "        #-----------------------------------------------------------------------\n",
        "        #\n",
        "        #                             Training mode\n",
        "        #\n",
        "        #-----------------------------------------------------------------------\n",
        "        if training_mode == True:\n",
        "\n",
        "          # The initial input for the decoder is ALWAYS a SOS token !\n",
        "          input = [[\"<SOS>\"] for el in range(len(outputs))]\n",
        "          input = inputToTensor(input, embedder).to(self.device)\n",
        "\n",
        "          # Making the predictions\n",
        "          for p in range(output_size):\n",
        "\n",
        "              #-----------------------------------------------------------------\n",
        "              #                             DECODER\n",
        "              #-----------------------------------------------------------------\n",
        "              # Make a prediction\n",
        "              current_prediction, current_hidden = self.decoder(input, hidden)\n",
        "\n",
        "              # Saves the current prediction(s)\n",
        "              for predic in range(len(current_prediction)):\n",
        "                predictions[predic, p, :] = current_prediction[predic, :]\n",
        "\n",
        "              #-----------------------------------------------------------------\n",
        "              #                            ATTENTION\n",
        "              #-----------------------------------------------------------------\n",
        "              # Computes attention if needed\n",
        "              if self.use_att == True:\n",
        "                current_hidden = self.attention(out_encoder, current_hidden)\n",
        "\n",
        "              # Update of the reccurent state for future predictions\n",
        "              hidden = current_hidden\n",
        "\n",
        "              #-----------------------------------------------------------------\n",
        "              #                 PREPARATION OF THE NEXT INPUT\n",
        "              #-----------------------------------------------------------------\n",
        "              # CASE 1 - No teacher forcing (No taining or simply not used)\n",
        "              if teacher_forcing_ratio == 0:\n",
        "                \n",
        "                    # The next input is the prediction made by the decoder\n",
        "                    input = getPredictedWord(current_prediction, embedder_IdToWord)\n",
        "                    input = inputToTensor(input, embedder).to(self.device)\n",
        "\n",
        "              # CASE 2 - Teacher forcing is activated\n",
        "              else:\n",
        "                    # Decides if teacher_force has to be used or not\n",
        "                    teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "                    # The input for the next prediction is the exact word\n",
        "                    if teacher_force:\n",
        "                      \n",
        "                      # The next input is the exact word\n",
        "                      input = outputs[:, p, :].unsqueeze(1)\n",
        "\n",
        "                    else:\n",
        "                      # The next input is the prediction made by the decoder\n",
        "                      input = getPredictedWord(current_prediction, embedder_IdToWord)\n",
        "                      input = inputToTensor(input, embedder).to(self.device)\n",
        "\n",
        "          return predictions\n",
        "\n",
        "        #-----------------------------------------------------------------------\n",
        "        #\n",
        "        #                                 Test mode\n",
        "        #\n",
        "        #-----------------------------------------------------------------------\n",
        "        if training_mode == False:\n",
        "\n",
        "          # Stores the predicted sentence\n",
        "          sentence = []\n",
        "\n",
        "          # The initial input for the decoder is ALWAYS a SOS token !\n",
        "          input = [[\"<SOS>\"]]\n",
        "          input = inputToTensor(input, embedder).to(self.device)\n",
        "\n",
        "          # Making the predictions\n",
        "          for p in range(output_size):\n",
        "\n",
        "              #-----------------------------------------------------------------\n",
        "              #                             DECODER\n",
        "              #-----------------------------------------------------------------\n",
        "              # Make a prediction\n",
        "              current_prediction, current_hidden = self.decoder(input, hidden)\n",
        "\n",
        "              # Saves the current prediction(s)\n",
        "              for predic in range(len(current_prediction)):\n",
        "                predictions[predic, p, :] = current_prediction[predic, :]\n",
        "\n",
        "              #-----------------------------------------------------------------\n",
        "              #                            ATTENTION\n",
        "              #-----------------------------------------------------------------\n",
        "              # Computes attention if needed\n",
        "              if self.use_att == True:\n",
        "                current_hidden = self.attention(out_encoder, current_hidden)\n",
        "\n",
        "              # Update of the reccurent state for future predictions\n",
        "              hidden = current_hidden\n",
        "\n",
        "              #-----------------------------------------------------------------\n",
        "              #                 PREPARATION OF THE NEXT INPUT\n",
        "              #-----------------------------------------------------------------\n",
        "              # The next input is the prediction made by the decoder\n",
        "              input = getPredictedWord(current_prediction, embedder_IdToWord)\n",
        "              sentence.append(input)\n",
        "              input = inputToTensor(input, embedder).to(self.device)\n",
        "\n",
        "          return sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mjaivF0dXtnI",
      "metadata": {
        "id": "mjaivF0dXtnI"
      },
      "source": [
        "### Documentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bap47NybX3n2",
      "metadata": {
        "id": "bap47NybX3n2"
      },
      "outputs": [],
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "#\n",
        "#                                   ENCODER\n",
        "#\n",
        "#-------------------------------------------------------------------------------\n",
        "#\n",
        "# input_size     = size of the vectors from the embedder\n",
        "#\n",
        "# hid_dimensions = size of the reccurent states\n",
        "#\n",
        "# num_layers     = number of GRUs stacked on one another\n",
        "#\n",
        "#------------\n",
        "# Shape Guide\n",
        "#------------\n",
        "#\n",
        "# embedded = [batch size, sequence length, embeder dimension]\n",
        "#\n",
        "# outputs  = [src len, batch size, hid dim * n directions]\n",
        "#\n",
        "# hidden   = [n layers * n directions, batch_size, hid dim]\n",
        "#\n",
        "#-------------------------------------------------------------------------------\n",
        "#\n",
        "#                                   DECODER\n",
        "#\n",
        "#-------------------------------------------------------------------------------\n",
        "#\n",
        "# input_size     = size of the vectors from the embedder\n",
        "#\n",
        "# hid_dimensions = size of the reccurent states (same as for encoder)\n",
        "#\n",
        "# output_size    = size of the output vector from the MLP\n",
        "#\n",
        "# num_layers     = number of GRUs stacked on one another in the DECODER ! It is important for shaping !\n",
        "#\n",
        "# NOTE : Les prédictions y faites par le décodeur sont y = sigma(h) donc la dimension\n",
        "#        de la prédiction est égale à hid_dimens\n",
        "#\n",
        "#------------\n",
        "# Shape Guide\n",
        "#------------\n",
        "#\n",
        "# input  = [batch size, seq_len, emb dim]\n",
        "#\n",
        "# output = [batch size, seq len, , hid dim * n directions]\n",
        "#\n",
        "# hidden = [n layers * n directions, batch size, hid dim]\n",
        "#\n",
        "# After MLP\n",
        "#\n",
        "# output = [batch size, output dim]\n",
        "#\n",
        "#-------------------------------------------------------------------------------\n",
        "#\n",
        "#                                     SBOT\n",
        "#\n",
        "#-------------------------------------------------------------------------------\n",
        "#--------------\n",
        "# Documentation\n",
        "#--------------\n",
        "# input            = input sentences  ([seq_len, batch_size])\n",
        "#\n",
        "# output_size      = number of words to be predicted\n",
        "#\n",
        "# embedder         = glove embedder\n",
        "#\n",
        "# embedder_reverse = Dic : ID ->  Word\n",
        "#\n",
        "# outputs          = List of correct words to be predicted (Used for\n",
        "#                    force teaching ONLY DURING TRAINING)\n",
        "#\n",
        "# teacher_forcing  = Probability to use teacher forcing\n",
        "#\n",
        "# Stores all the predictions made by the SBOT ! The shape of it is :\n",
        "#\n",
        "#            [batch_size, output_size, size_HP_vocabulary]\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qHvKq-nYZyBn",
      "metadata": {
        "id": "qHvKq-nYZyBn"
      },
      "source": [
        "#--------------------------------------------------------------------------------------------------------------------------------------------- \n",
        "                                                        Training of the SBOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UcvUby3_B38e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-03T12:10:44.278414Z",
          "iopub.status.busy": "2022-05-03T12:10:44.277612Z",
          "iopub.status.idle": "2022-05-03T12:10:44.282040Z",
          "shell.execute_reply": "2022-05-03T12:10:44.281477Z",
          "shell.execute_reply.started": "2022-05-03T12:10:44.278345Z"
        },
        "id": "UcvUby3_B38e"
      },
      "outputs": [],
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "#                              Training parameters\n",
        "#-------------------------------------------------------------------------------\n",
        "# Define if the sbot trained is new or must be loaded\n",
        "new_sbot = True\n",
        "\n",
        "# Number of the model to load \n",
        "sbot_index = 0\n",
        "\n",
        "# Path to the correct folder to save the model and results\n",
        "folder_path = path + \"Models/\" + user_path + \"/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HcRRmpwlJcn4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-03T12:11:54.550789Z",
          "iopub.status.busy": "2022-05-03T12:11:54.549730Z",
          "iopub.status.idle": "2022-05-03T12:12:00.426151Z",
          "shell.execute_reply": "2022-05-03T12:12:00.425646Z",
          "shell.execute_reply.started": "2022-05-03T12:11:54.550715Z"
        },
        "id": "HcRRmpwlJcn4"
      },
      "outputs": [],
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "#                           Intialization of the SBOT\n",
        "#-------------------------------------------------------------------------------\n",
        "if new_sbot == True:\n",
        "\n",
        "  sbot = SBOT_V2(50, hidden_size, len(embedder_IdToWord), num_layers_encoder, num_layers_decoder, device, dropout, bidirectional, use_attention, input_size)\n",
        "\n",
        "else:\n",
        "  \n",
        "  results_dic = loadTrainingInfo(path + \"Models/\" + user_path + \"/\", sbot_index)\n",
        "  results     = loadTrainingParameters(results_dic)\n",
        "  sbot = SBOT_V2(50, results[4], len(embedder_IdToWord), results[2], results[3], device, dropout, results[9], results[6], results[7])\n",
        "\n",
        "total_params = sum(p.numel() for p in sbot.parameters() if p.requires_grad)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(sbot.parameters(), lr = learning_rate)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', factor = 0.5, patience = 5, threshold = 1e-2)\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "#                                Loading the SBOT\n",
        "#-------------------------------------------------------------------------------\n",
        "if new_sbot == False:\n",
        "  sbot, optimizer = loadModel(path + \"Models/\" + user_path + \"/\", sbot, optimizer, sbot_index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nSTtSAoHVcS0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-03T12:12:25.493037Z",
          "iopub.status.busy": "2022-05-03T12:12:25.492626Z",
          "iopub.status.idle": "2022-05-03T12:12:27.885954Z",
          "shell.execute_reply": "2022-05-03T12:12:27.885065Z",
          "shell.execute_reply.started": "2022-05-03T12:12:25.493007Z"
        },
        "id": "nSTtSAoHVcS0"
      },
      "outputs": [],
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "#                                  Training\n",
        "#-------------------------------------------------------------------------------\n",
        "# First training information\n",
        "losses_train_total = []\n",
        "losses_valid_total = []\n",
        "offset_epoch = 0\n",
        "\n",
        "# Adjusting training information\n",
        "if new_sbot == False:\n",
        "  losses_train_total = results[0]\n",
        "  losses_valid_total = results[1]\n",
        "  offset_epoch = results[10]\n",
        "\n",
        "# Displaying all the parameters of the current model undergoing training\n",
        "if new_sbot == True:\n",
        "  trainingInfo(num_layers_encoder, num_layers_decoder, hidden_size, total_params, teacher_forcing_ratio, bidirectional, nb_epoch, batch_size, use_attention, input_size)\n",
        "else:\n",
        "  trainingInfo(results[2], results[3], results[4], results[5], results[8], results[9], results[10] + offset_epoch, results[11], results[6], results[7])\n",
        "\n",
        "# Number of output to be produced by the decoder\n",
        "output_size = max_sentence_length - input_size\n",
        "\n",
        "# Other parameters\n",
        "t_size = len(X_train)\n",
        "v_size = len(X_valid)\n",
        "estimated_time = 0\n",
        "\n",
        "# Going through epochs\n",
        "for epoch in range(nb_epoch):\n",
        "\n",
        "    # Used to compute the average loss value over the all epoch\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "\n",
        "    # Display useful information\n",
        "    print(\"\\nEpoch : \", epoch + 1 + offset_epoch, \"/\", nb_epoch + offset_epoch, \"\\n\")\n",
        "    index = batch_size\n",
        "    start = time.time()\n",
        "\n",
        "    #---------------------------------------------------------------------------\n",
        "    #                                   Training\n",
        "    #---------------------------------------------------------------------------\n",
        "    for x, y, y_iD, y_mask in HP_train:\n",
        "\n",
        "        # Passing the data to the GPU\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        y_iD = y_iD.to(device)\n",
        "\n",
        "        # Computing SBOT prediction\n",
        "        pred = sbot(x, output_size, embedder, embedder_IdToWord, y, teacher_forcing_ratio, training_mode = True)\n",
        "        \n",
        "        # Computing the loss\n",
        "        loss = criterion(pred[y_mask], y_iD[y_mask]) \n",
        "\n",
        "        # Adding the loss\n",
        "        train_losses.append(loss.detach().item())\n",
        "\n",
        "        # Reseting the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Optimizing the parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Removing data from the GPU\n",
        "        x.to('cpu')\n",
        "        y.to('cpu') \n",
        "        y_iD.to('cpu')\n",
        "\n",
        "        # Update the progress bar\n",
        "        time_left = estimated_time - (time.time() - start)\n",
        "        progressBar(loss, 0, time_left, (index/t_size)*100)\n",
        "        index = index + batch_size\n",
        "\n",
        "    # Computing mean loss\n",
        "    mean_loss = sum(train_losses)/len(train_losses)\n",
        "    losses_train_total.append(mean_loss)\n",
        "\n",
        "    # Display useful information\n",
        "    estimated_time = time.time() - start\n",
        "    progressBar(mean_loss, 0, 0, 100)\n",
        "\n",
        "    #---------------------------------------------------------------------------\n",
        "    #                                 Validation\n",
        "    #---------------------------------------------------------------------------\n",
        "    index_validation = batch_size\n",
        "\n",
        "    with torch.no_grad():  \n",
        "        \n",
        "        for x, y, y_iD, y_mask in HP_valid:\n",
        "\n",
        "          # Passing the data to the GPU\n",
        "          x = x.to(device)\n",
        "          y = y.to(device)\n",
        "          y_iD = y_iD.to(device)\n",
        "\n",
        "          # Computing SBOT prediction\n",
        "          pred = sbot(x, output_size, embedder, embedder_IdToWord, y, teacher_forcing_ratio, training_mode = True)\n",
        "          \n",
        "          # Computing the loss\n",
        "          loss_2 = criterion(pred[y_mask], y_iD[y_mask]) \n",
        "\n",
        "          # Adding the loss\n",
        "          valid_losses.append(loss_2.detach().item())\n",
        "          \n",
        "          # Removing data from the GPU\n",
        "          x.to('cpu')\n",
        "          y.to('cpu') \n",
        "          y_iD.to('cpu')\n",
        "\n",
        "          # Update the progress bar\n",
        "          progressBar(mean_loss, loss_2, time_left, (index_validation/v_size)*100)\n",
        "          index_validation = index_validation + batch_size\n",
        "\n",
        "    # Update of the scheduler\n",
        "    scheduler.step(mean_loss)\n",
        "    \n",
        "    # Computing mean loss\n",
        "    mean_loss_2 = sum(valid_losses)/len(valid_losses)\n",
        "\n",
        "    # Adding the final loss of the epoch\n",
        "    losses_valid_total.append(mean_loss_2)\n",
        "\n",
        "    # Display useful information\n",
        "    estimated_time = time.time() - start\n",
        "    progressBar(mean_loss, mean_loss_2, 0, 100)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    if (epoch + 1) in checkpoints:\n",
        "      \n",
        "      # Saving the results\n",
        "      saveTrainingInfo(folder_path, losses_train_total, losses_valid_total, \n",
        "                      [num_layers_encoder, num_layers_decoder, hidden_size, total_params, use_attention, input_size, teacher_forcing_ratio, bidirectional, nb_epoch + offset_epoch, batch_size])\n",
        "\n",
        "      saveModel(folder_path, sbot, optimizer)\n",
        "\n",
        "# Information over terminal\n",
        "print(\"---------------------------------------------------------------------\")\n",
        "print(\"                 Finish training and model saved                     \")\n",
        "print(\"---------------------------------------------------------------------\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [
        "nZUjC1T3S9b6",
        "tqosjvczI9Zo",
        "3f323e18",
        "48719bc6",
        "gd9BjZLcAGj9",
        "a8KzsI6EKyQ4",
        "JSmC4gng8NV8",
        "QRIVhKaBa3vV",
        "qjMgFPsB4GFG",
        "PAl9QgyaXk31",
        "Emp62PW1SvTJ",
        "2_mtU9uia8ry",
        "-SATueBMMv4R",
        "sun1vN_gdbU6",
        "YKbgkbKv6vwT",
        "ke6Xx20OaPSy",
        "VzoeJ98gpqEr",
        "mjaivF0dXtnI",
        "qHvKq-nYZyBn"
      ],
      "name": "SBOTV2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}